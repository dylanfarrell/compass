{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File for implementing Naive Bayes and random forest with bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielalpert/anaconda/lib/python2.7/site-packages/matplotlib/colors.py:680: MatplotlibDeprecationWarning: The is_string_like function was deprecated in version 2.1.\n",
      "  not cbook.is_string_like(colors[0]):\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import collections, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import treeUtil\n",
    "import cPickle\n",
    "from math import log\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielalpert/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn\n",
    "import sklearn.multiclass\n",
    "import sklearn.svm\n",
    "import sklearn.cross_validation\n",
    "import sklearn.linear_model\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.externals.joblib\n",
    "import sklearn.metrics\n",
    "import sklearn.feature_extraction.text\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import utils\n",
    "import argparse\n",
    "import h5py\n",
    "if utils.is_python3():\n",
    "    import configparser as ConfigParser\n",
    "else:\n",
    "    import ConfigParser\n",
    "    import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat = pd.read_excel('all_sentences.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5372, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>words</th>\n",
       "      <th>words_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0</td>\n",
       "      <td>Forcing middle-class workers to bear a greater...</td>\n",
       "      <td>forcing middle class workers bear greater shar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>Because it would not be worthwhile to bring a ...</td>\n",
       "      <td>would worthwhile bring case arbitration clause...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0</td>\n",
       "      <td>Indeed , Lind argues that high profits and hig...</td>\n",
       "      <td>indeed lind argues high profits high wages rei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0</td>\n",
       "      <td>In fairness , it should be noted that he devot...</td>\n",
       "      <td>fairness noted devotes entire chapter new york...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0</td>\n",
       "      <td>Psychological tactics are social control techn...</td>\n",
       "      <td>psychological tactics social control technique...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                              words  \\\n",
       "0.0      0  Forcing middle-class workers to bear a greater...   \n",
       "1.0      0  Because it would not be worthwhile to bring a ...   \n",
       "2.0      0  Indeed , Lind argues that high profits and hig...   \n",
       "3.0      0  In fairness , it should be noted that he devot...   \n",
       "4.0      0  Psychological tactics are social control techn...   \n",
       "\n",
       "                                           words_clean  \n",
       "0.0  forcing middle class workers bear greater shar...  \n",
       "1.0  would worthwhile bring case arbitration clause...  \n",
       "2.0  indeed lind argues high profits high wages rei...  \n",
       "3.0  fairness noted devotes entire chapter new york...  \n",
       "4.0  psychological tactics social control technique...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dat.shape)\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to clean word data--removes stopwords, makes lowercase, removes numbers\n",
    "def clean_words(sentences):\n",
    "    words_clean = np.full(len(sentences), None)\n",
    "    for i, words in enumerate(sentences):\n",
    "        word_list = re.split('\\W+', words)\n",
    "        words1 = [word.lower() for word in word_list if word.lower() not in stopwords.words('english')]\n",
    "        words2 = [word for word in words1 if not any(char.isdigit() for char in word)]\n",
    "        words_clean[i] = (' '.join(words2)).strip()\n",
    "    return words_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4029, 102236)\n",
      "(1343, 102236)\n",
      "(4029,)\n",
      "(1343,)\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(dat['words_clean'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, dat['label'], test_size=0.25, random_state=12)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct labels out of 1343 points : 755\n",
      "Accuracy: 0.562174236783\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "nb_model = mnb.fit(X_train, y_train)\n",
    "y_pred = nb_model.predict(X_test)\n",
    "#y_pred = mnb.fit(X_train, train['label']).predict(X_test)\n",
    "print(\"Correct labels out of %d points : %d\"\n",
    "      % (X_test.shape[0],(y_test == y_pred).sum()))\n",
    "print('Accuracy:', (y_test == y_pred).sum()/X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57110945644080413"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = nb_model.predict_proba(X_test)\n",
    "y_pred = []\n",
    "for row in probs:\n",
    "    if row[1] > 0.31:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(np.argmax(row))\n",
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top10(vectorizer, clf, class_labels):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \" \".join(feature_names[j] for j in top10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: economic new energy health public people government care tax would\n",
      "1: make good us even people jerusalem new one said would\n",
      "2: state new federal make economic one free people would government\n"
     ]
    }
   ],
   "source": [
    "print_top10(vectorizer, nb_model, [0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_acc(probs, y_test, thresh):\n",
    "    y_pred = []\n",
    "    for row in probs:\n",
    "        if row[1] > thresh:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(np.argmax(row))\n",
    "    #print(len(y_pred), len(y_test))\n",
    "    return y_pred, np.mean(np.array(y_pred) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "nb_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))), \n",
    "                     ('tfidf', TfidfTransformer()), ('clf', MultinomialNB()),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune accuracy parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23641439205955334, 0.23672456575682382, 0.24317617866004956, 0.2616935483870968, 0.29255583126550866, 0.33455334987593061, 0.37630272952853588, 0.41861042183622832, 0.4561414392059554, 0.4850806451612904, 0.50660669975186101, 0.52651985111662536, 0.54209057071960287, 0.55158188585607937, 0.55496277915632741, 0.55614143920595538, 0.55719602977667493, 0.55514888337468993, 0.5533498759305211, 0.55046526054590572, 0.54621588089330031, 0.54311414392059554, 0.53911290322580641, 0.5349565756823822, 0.53176178660049622, 0.52844292803970216, 0.52546526054590559, 0.52264267990074453, 0.51932382133995048, 0.51656327543424307, 0.51395781637717108, 0.51225186104218368, 0.51057692307692304, 0.50896401985111672, 0.50710297766749379, 0.50511786600496278, 0.50397022332506203, 0.50341191066997515, 0.50300868486352346, 0.50266749379652609, 0.50260545905707188, 0.50254342431761789, 0.50248138957816368, 0.50248138957816368, 0.50248138957816368, 0.50248138957816368, 0.50248138957816368, 0.50248138957816368, 0.50248138957816368, 0.50248138957816368]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = [0]*50\n",
    "for j in range(20):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dat['words_clean'], dat['label'], test_size=0.3, random_state=j)\n",
    "    nb_model.fit(X_train, y_train) \n",
    "    threshes = np.arange(0.0, 0.5, 0.01)\n",
    "    for i, thresh in enumerate(threshes):\n",
    "        probs = nb_model.predict_proba(X_test)\n",
    "        accuracy[i] += return_acc(probs, y_test, thresh)[1]\n",
    "\n",
    "new_acc = [acc/20 for acc in accuracy]\n",
    "print(new_acc)\n",
    "np.argmax(new_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55514888337468993"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_acc[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'popall'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-637227ff296c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Neutral Threshold vs. Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Neutral threshold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.17\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielalpert/anaconda/lib/python2.7/site-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/Users/danielalpert/anaconda/lib/python2.7/site-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mgca\u001b[0;34m(**kwargs)\u001b[0m\n",
      "\u001b[0;32m/Users/danielalpert/anaconda/lib/python2.7/site-packages/matplotlib/figure.pyc\u001b[0m in \u001b[0;36mgca\u001b[0;34m(self, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/Users/danielalpert/anaconda/lib/python2.7/site-packages/matplotlib/figure.pyc\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/Users/danielalpert/anaconda/lib/python2.7/site-packages/matplotlib/axes/_subplots.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/Users/danielalpert/anaconda/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, rect, facecolor, frameon, sharex, sharey, label, xscale, yscale, axisbg, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/Users/danielalpert/anaconda/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36m_init_axis\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/Users/danielalpert/anaconda/lib/python2.7/site-packages/matplotlib/axis.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, axes, pickradius)\u001b[0m\n",
      "\u001b[0;32m/Users/danielalpert/anaconda/lib/python2.7/site-packages/matplotlib/axis.pyc\u001b[0m in \u001b[0;36mcla\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/Users/danielalpert/anaconda/lib/python2.7/site-packages/matplotlib/axis.pyc\u001b[0m in \u001b[0;36mreset_ticks\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'popall'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11233eb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(threshes, new_acc)\n",
    "plt.title('Neutral Threshold vs. Accuracy')\n",
    "plt.xlabel('Neutral threshold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.axvline(x=0.17, c='r', linewidth=0.5, ls='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_acc = [None]*4\n",
    "for i in range(len(avg_acc)):\n",
    "    nb_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,i+1))), \n",
    "                     ('tfidf', TfidfTransformer()), ('clf', MultinomialNB()),])\n",
    "    #print(nb_model_2)\n",
    "    accuracy = [0]*20\n",
    "    for j in range(len(accuracy)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dat['words_clean'], dat['label'], test_size=0.3, random_state=j)\n",
    "        nb_model.fit(X_train, y_train) \n",
    "        probs = nb_model.predict_proba(X_test)\n",
    "        accuracy[j] += return_acc(probs, y_test, 0.17)[1]\n",
    "    avg_acc[i] = sum(accuracy)/len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.56498138957816368,\n",
       " 0.55514888337468993,\n",
       " 0.55117866004962768,\n",
       " 0.54900744416873448]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.532878411911\n",
      "0.717741935484\n"
     ]
    }
   ],
   "source": [
    "# Best model\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat['words_clean'], dat['label'], test_size=0.3)\n",
    "nb_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))), \n",
    "                     ('tfidf', TfidfTransformer()), ('clf', MultinomialNB()),])\n",
    "nb_model.fit(X_train, y_train)\n",
    "probs = nb_model.predict_proba(X_test)\n",
    "ret = return_acc(probs, y_test, 0.17)\n",
    "print(ret[1])\n",
    "print((abs(y_test - ret[0]) <= 1).sum()/X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test sentences\n",
    "text = ['Democrats are the best party', 'Trump is a great president', 'Red sox won again', \n",
    "        'I eat at burger king twice a week', 'This bill has the potential to hurt housing for low-income minorities',\n",
    "        'By carrying this out, the american people are attacking freedom', 'black lives matter', 'blue lives matter',\n",
    "        'white lives matter', 'trickle down economics saved our economy', 'regulations saved our economy',\n",
    "        'illegal immigrants actually help our economy more than hurt it', 'i am neutral 6']\n",
    "labels = pd.Series([0,2,1,1,0,2,0,2,2,2,0,0,1])\n",
    "clean_text = clean_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 1, 1, 0, 2, 0, 1, 0, 0, 0, 0, 1], 0.69230769230769229)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = nb_model.predict_proba(clean_text)\n",
    "#print(type(y_pred), type(labels.tolist()))\n",
    "return_acc(probs, labels, 0.17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.053439099296746284,\n",
       " 0.25453105286842331,\n",
       " 0.028709354909863261,\n",
       " 0.14745926939266624,\n",
       " 0.39219901660444884,\n",
       " -0.031046310260656573,\n",
       " 0.2695985764193724,\n",
       " 0.14205940778550102,\n",
       " 0.21644608982092911,\n",
       " 0.21865346741242619,\n",
       " 0.037614802880942377,\n",
       " 0.24418034849431458,\n",
       " 0.17753877046382754]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs = [row[0]-row[2] for row in probs]\n",
    "diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        st...False,\n",
       "         use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dat['words_clean'], dat['label'], test_size=0.2, random_state=0)\n",
    "nb_model_3 = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), max_features=1000)), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB()),])\n",
    "nb_model_3.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53597278477500976"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clf = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(nb_model_3, dat['words_clean'], dat['label'], cv=10)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrats are the best party\n",
      "Trump is a great president\n",
      "Red sox won again\n",
      "I eat at burger king twice a week\n",
      "This bill has the potential to hurt housing for low-income minorities\n",
      "By carrying this out, the american people are attacking freedom\n",
      "black lives matter\n",
      "blue lives matter\n",
      "white lives matter\n",
      "trickle down economics saved our economy\n",
      "regulations saved our economy\n",
      "illegal immigrants actually help our economy more than hurt it\n",
      "i am neutral\n",
      "invertebrates have the highest acidity in the animal kingdom\n",
      "hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['democrats best party',\n",
       " 'trump great president',\n",
       " 'red sox',\n",
       " 'eat burger king twice week',\n",
       " 'bill potential hurt housing low-income minorities',\n",
       " 'carrying out, american people attacking freedom',\n",
       " 'black lives matter',\n",
       " 'blue lives matter',\n",
       " 'white lives matter',\n",
       " 'trickle economics saved economy',\n",
       " 'regulations saved economy',\n",
       " 'illegal immigrants actually help economy hurt',\n",
       " 'neutral',\n",
       " 'invertebrates highest acidity animal kingdom',\n",
       " 'hello']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['Democrats are the best party', 'Trump is a great president', 'Red sox won again', \n",
    "        'I eat at burger king twice a week', 'This bill has the potential to hurt housing for low-income minorities',\n",
    "        'By carrying this out, the american people are attacking freedom', 'black lives matter', 'blue lives matter',\n",
    "        'white lives matter', 'trickle down economics saved our economy', 'regulations saved our economy',\n",
    "        'illegal immigrants actually help our economy more than hurt it', 'i am neutral',\n",
    "        'invertebrates have the highest acidity in the animal kingdom', 'hello']\n",
    "new_text = []\n",
    "for string in text:\n",
    "    print(string)\n",
    "    words = (string.lower()).split(' ')\n",
    "    new_text.append(' '.join([word for word in words if word not in stopwords.words('english')]))\n",
    "\n",
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = nb_model_3.predict_proba(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dat['words_clean'], dat['label'], test_size=0.3, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,1), max_features=500)), \n",
    "                     ('tfidf', TfidfTransformer()), ('clf', tree.DecisionTreeClassifier(max_depth=500)),])\n",
    "tree_model.fit(X_train, y_train)\n",
    "y_pred = tree_model.predict(X_test)\n",
    "probs = tree_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49007444168734493"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_acc(probs, y_test, .34)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48901985111662538, 0.48933002481389576, 0.48933002481389576, 0.48933002481389576, 0.48933002481389576, 0.48933002481389576, 0.48933002481389576, 0.48933002481389576, 0.48933002481389576, 0.48933002481389576, 0.48933002481389576, 0.48933002481389576, 0.48933002481389576, 0.48933002481389576, 0.48933002481389576, 0.48933002481389576, 0.48933002481389576]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = [0]*50\n",
    "for j in range(20):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dat['words_clean'], dat['label'], test_size=0.3, random_state=j)\n",
    "    tree_model.fit(X_train, y_train) \n",
    "    threshes = np.arange(0.0, 0.5, 0.01)\n",
    "    for i, thresh in enumerate(threshes):\n",
    "        probs = tree_model.predict_proba(X_test)\n",
    "        accuracy[i] += return_acc(probs, y_test, thresh)[1]\n",
    "\n",
    "new_acc = [acc/20 for acc in accuracy]\n",
    "print(new_acc)\n",
    "np.argmax(new_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_feats = [10,30,50,100,200,300,500,800,1000,1500]\n",
    "avg_acc = [None]*len(num_feats)\n",
    "for i, feats in enumerate(num_feats):\n",
    "    tree_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,1), max_features=feats)), \n",
    "                     ('tfidf', TfidfTransformer()), ('clf', tree.DecisionTreeClassifier(max_depth=500)),])\n",
    "    #print(nb_model_2)\n",
    "    accuracy = [0]*20\n",
    "    for j in range(len(accuracy)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dat['words_clean'], dat['label'], test_size=0.3, random_state=j)\n",
    "        tree_model.fit(X_train, y_train) \n",
    "        probs = tree_model.predict_proba(X_test)\n",
    "        accuracy[j] += return_acc(probs, y_test, 0.34)[1]\n",
    "    avg_acc[i] = sum(accuracy)/len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42788461538461531,\n",
       " 0.42673697270471467,\n",
       " 0.44630893300248137,\n",
       " 0.45908808933002482,\n",
       " 0.47354218362282874,\n",
       " 0.48067617866004958,\n",
       " 0.48725186104218371,\n",
       " 0.49230769230769234,\n",
       " 0.49441687344913143,\n",
       " 0.48874069478908194]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dat['words_clean'], dat['label'], test_size=0.3, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56079404466501237"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,1), max_features=1500)), \n",
    "                     ('tfidf', TfidfTransformer()), ('clf', ensemble.RandomForestClassifier(max_depth=200, n_estimators=50)),])\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25682382134\n",
      "0.25682382134\n",
      "0.285359801489\n",
      "0.287841191067\n",
      "0.323200992556\n",
      "0.326302729529\n",
      "0.357320099256\n",
      "0.364143920596\n",
      "0.383995037221\n",
      "0.390818858561\n",
      "0.41253101737\n",
      "0.424937965261\n",
      "0.444168734491\n",
      "0.458436724566\n",
      "0.468362282878\n",
      "0.483250620347\n",
      "0.486972704715\n",
      "0.494416873449\n",
      "0.501240694789\n",
      "0.50682382134\n",
      "0.514267990074\n",
      "0.519851116625\n",
      "0.526054590571\n",
      "0.532878411911\n",
      "0.5341191067\n",
      "0.536600496278\n",
      "0.543424317618\n",
      "0.549007444169\n",
      "0.549627791563\n",
      "0.55459057072\n",
      "0.557692307692\n",
      "0.56017369727\n",
      "0.56141439206\n",
      "0.565136476427\n",
      "0.564516129032\n",
      "0.566377171216\n",
      "0.564516129032\n",
      "0.566377171216\n",
      "0.565136476427\n",
      "0.565136476427\n",
      "0.566377171216\n",
      "0.566377171216\n",
      "0.565756823821\n",
      "0.564516129032\n",
      "0.563275434243\n",
      "0.563275434243\n",
      "0.563275434243\n",
      "0.563275434243\n",
      "0.563275434243\n",
      "0.563275434243\n",
      "0.239454094293\n",
      "0.239454094293\n",
      "0.274193548387\n",
      "0.276054590571\n",
      "0.305210918114\n",
      "0.31141439206\n",
      "0.341191066998\n",
      "0.349875930521\n",
      "0.382133995037\n",
      "0.393920595533\n",
      "0.414392059553\n",
      "0.422456575682\n",
      "0.444168734491\n",
      "0.451612903226\n",
      "0.468362282878\n",
      "0.478908188586\n",
      "0.488833746898\n",
      "0.498138957816\n",
      "0.504962779156\n",
      "0.510545905707\n",
      "0.518610421836\n",
      "0.523573200993\n",
      "0.528535980149\n",
      "0.535359801489\n",
      "0.542183622829\n",
      "0.547146401985\n",
      "0.552729528536\n",
      "0.555210918114\n",
      "0.55459057072\n",
      "0.555831265509\n",
      "0.56141439206\n",
      "0.56017369727\n",
      "0.560794044665\n",
      "0.562034739454\n",
      "0.565756823821\n",
      "0.568858560794\n",
      "0.568858560794\n",
      "0.572580645161\n",
      "0.574441687345\n",
      "0.576302729529\n",
      "0.576302729529\n",
      "0.576302729529\n",
      "0.575682382134\n",
      "0.574441687345\n",
      "0.575062034739\n",
      "0.575062034739\n",
      "0.575062034739\n",
      "0.575062034739\n",
      "0.575682382134\n",
      "0.575682382134\n",
      "0.264267990074\n",
      "0.266129032258\n",
      "0.291563275434\n",
      "0.293424317618\n",
      "0.320719602978\n",
      "0.324441687345\n",
      "0.350496277916\n",
      "0.359181141439\n",
      "0.383374689826\n",
      "0.392679900744\n",
      "0.408808933002\n",
      "0.415632754342\n",
      "0.436724565757\n",
      "0.447270471464\n",
      "0.455955334988\n",
      "0.467121588089\n",
      "0.477047146402\n",
      "0.482630272953\n",
      "0.49317617866\n",
      "0.501240694789\n",
      "0.511166253102\n",
      "0.517990074442\n",
      "0.524813895782\n",
      "0.52729528536\n",
      "0.529156327543\n",
      "0.531637717122\n",
      "0.532258064516\n",
      "0.533498759305\n",
      "0.536600496278\n",
      "0.542183622829\n",
      "0.546526054591\n",
      "0.545905707196\n",
      "0.54776674938\n",
      "0.551488833747\n",
      "0.552109181141\n",
      "0.551488833747\n",
      "0.552729528536\n",
      "0.555210918114\n",
      "0.558312655087\n",
      "0.558312655087\n",
      "0.558312655087\n",
      "0.557692307692\n",
      "0.556451612903\n",
      "0.555831265509\n",
      "0.555210918114\n",
      "0.555831265509\n",
      "0.556451612903\n",
      "0.557071960298\n",
      "0.557071960298\n",
      "0.557071960298\n",
      "0.247518610422\n",
      "0.248138957816\n",
      "0.282878411911\n",
      "0.284739454094\n",
      "0.320719602978\n",
      "0.325062034739\n",
      "0.355459057072\n",
      "0.362903225806\n",
      "0.384615384615\n",
      "0.393300248139\n",
      "0.411290322581\n",
      "0.423076923077\n",
      "0.439205955335\n",
      "0.446650124069\n",
      "0.455334987593\n",
      "0.466501240695\n",
      "0.478287841191\n",
      "0.484491315136\n",
      "0.496898263027\n",
      "0.506203473945\n",
      "0.516749379653\n",
      "0.524193548387\n",
      "0.527915632754\n",
      "0.530397022333\n",
      "0.536600496278\n",
      "0.541563275434\n",
      "0.545905707196\n",
      "0.550248138958\n",
      "0.555210918114\n",
      "0.560794044665\n",
      "0.565136476427\n",
      "0.566377171216\n",
      "0.56699751861\n",
      "0.569478908189\n",
      "0.570099255583\n",
      "0.572580645161\n",
      "0.576923076923\n",
      "0.577543424318\n",
      "0.576302729529\n",
      "0.576923076923\n",
      "0.578163771712\n",
      "0.579404466501\n",
      "0.579404466501\n",
      "0.578784119107\n",
      "0.578784119107\n",
      "0.578784119107\n",
      "0.578784119107\n",
      "0.578784119107\n",
      "0.578163771712\n",
      "0.578163771712\n",
      "0.264267990074\n",
      "0.266129032258\n",
      "0.299627791563\n",
      "0.302729528536\n",
      "0.333746898263\n",
      "0.339950372208\n",
      "0.365384615385\n",
      "0.368486352357\n",
      "0.393300248139\n",
      "0.403846153846\n",
      "0.4317617866\n",
      "0.443548387097\n",
      "0.450992555831\n",
      "0.457816377171\n",
      "0.467121588089\n",
      "0.477667493797\n",
      "0.488833746898\n",
      "0.499379652605\n",
      "0.505583126551\n",
      "0.51364764268\n",
      "0.515508684864\n",
      "0.522952853598\n",
      "0.531017369727\n",
      "0.537841191067\n",
      "0.541563275434\n",
      "0.546526054591\n",
      "0.545905707196\n",
      "0.549627791563\n",
      "0.548387096774\n",
      "0.549007444169\n",
      "0.546526054591\n",
      "0.546526054591\n",
      "0.546526054591\n",
      "0.549007444169\n",
      "0.550868486352\n",
      "0.548387096774\n",
      "0.549627791563\n",
      "0.549007444169\n",
      "0.550868486352\n",
      "0.551488833747\n",
      "0.552109181141\n",
      "0.551488833747\n",
      "0.552729528536\n",
      "0.553349875931\n",
      "0.553349875931\n",
      "0.553349875931\n",
      "0.553349875931\n",
      "0.553349875931\n",
      "0.553349875931\n",
      "0.553349875931\n",
      "0.271091811414\n",
      "0.271712158809\n",
      "0.307692307692\n",
      "0.310794044665\n",
      "0.33746898263\n",
      "0.343672456576\n",
      "0.366004962779\n",
      "0.374069478908\n",
      "0.397022332506\n",
      "0.406327543424\n",
      "0.43300248139\n",
      "0.441687344913\n",
      "0.448511166253\n",
      "0.457816377171\n",
      "0.469602977667\n",
      "0.480769230769\n",
      "0.485732009926\n",
      "0.494416873449\n",
      "0.501240694789\n",
      "0.508064516129\n",
      "0.519851116625\n",
      "0.522952853598\n",
      "0.526674937965\n",
      "0.5341191067\n",
      "0.538461538462\n",
      "0.541563275434\n",
      "0.542183622829\n",
      "0.544044665012\n",
      "0.54776674938\n",
      "0.550248138958\n",
      "0.551488833747\n",
      "0.557692307692\n",
      "0.557692307692\n",
      "0.555210918114\n",
      "0.556451612903\n",
      "0.557071960298\n",
      "0.557692307692\n",
      "0.557071960298\n",
      "0.553349875931\n",
      "0.551488833747\n",
      "0.552109181141\n",
      "0.552729528536\n",
      "0.55459057072\n",
      "0.555210918114\n",
      "0.553970223325\n",
      "0.553349875931\n",
      "0.553349875931\n",
      "0.553349875931\n",
      "0.553349875931\n",
      "0.553349875931\n",
      "0.266129032258\n",
      "0.269230769231\n",
      "0.303970223325\n",
      "0.307692307692\n",
      "0.334367245658\n",
      "0.338089330025\n",
      "0.372828784119\n",
      "0.382754342432\n",
      "0.406327543424\n",
      "0.414392059553\n",
      "0.434863523573\n",
      "0.447270471464\n",
      "0.455955334988\n",
      "0.467741935484\n",
      "0.477047146402\n",
      "0.488833746898\n",
      "0.500620347395\n",
      "0.507444168734\n",
      "0.51364764268\n",
      "0.518610421836\n",
      "0.525434243176\n",
      "0.531017369727\n",
      "0.538461538462\n",
      "0.544665012407\n",
      "0.545905707196\n",
      "0.547146401985\n",
      "0.548387096774\n",
      "0.550248138958\n",
      "0.553349875931\n",
      "0.558933002481\n",
      "0.559553349876\n",
      "0.560794044665\n",
      "0.563895781638\n",
      "0.567617866005\n",
      "0.574441687345\n",
      "0.574441687345\n",
      "0.574441687345\n",
      "0.576302729529\n",
      "0.576923076923\n",
      "0.579404466501\n",
      "0.580024813896\n",
      "0.580024813896\n",
      "0.578784119107\n",
      "0.578784119107\n",
      "0.579404466501\n",
      "0.579404466501\n",
      "0.578784119107\n",
      "0.578784119107\n",
      "0.578784119107\n",
      "0.578784119107\n",
      "0.25\n",
      "0.250620347395\n",
      "0.286600496278\n",
      "0.288461538462\n",
      "0.322580645161\n",
      "0.325062034739\n",
      "0.346153846154\n",
      "0.358560794045\n",
      "0.37841191067\n",
      "0.388337468983\n",
      "0.40570719603\n",
      "0.413151364764\n",
      "0.429900744417\n",
      "0.439205955335\n",
      "0.457816377171\n",
      "0.465260545906\n",
      "0.481389578164\n",
      "0.488213399504\n",
      "0.498138957816\n",
      "0.50682382134\n",
      "0.522952853598\n",
      "0.529156327543\n",
      "0.534739454094\n",
      "0.540322580645\n",
      "0.543424317618\n",
      "0.549007444169\n",
      "0.555210918114\n",
      "0.555831265509\n",
      "0.555210918114\n",
      "0.558312655087\n",
      "0.559553349876\n",
      "0.563895781638\n",
      "0.563275434243\n",
      "0.565136476427\n",
      "0.565136476427\n",
      "0.565756823821\n",
      "0.5682382134\n",
      "0.5682382134\n",
      "0.568858560794\n",
      "0.568858560794\n",
      "0.567617866005\n",
      "0.5682382134\n",
      "0.567617866005\n",
      "0.56699751861\n",
      "0.5682382134\n",
      "0.567617866005\n",
      "0.567617866005\n",
      "0.567617866005\n",
      "0.56699751861\n",
      "0.56699751861\n",
      "0.248138957816\n",
      "0.248759305211\n",
      "0.279156327543\n",
      "0.281017369727\n",
      "0.320099255583\n",
      "0.321960297767\n",
      "0.352357320099\n",
      "0.361042183623\n",
      "0.386476426799\n",
      "0.390818858561\n",
      "0.408808933002\n",
      "0.415012406948\n",
      "0.429280397022\n",
      "0.436724565757\n",
      "0.446029776675\n",
      "0.454714640199\n",
      "0.467741935484\n",
      "0.472084367246\n",
      "0.477667493797\n",
      "0.483870967742\n",
      "0.493796526055\n",
      "0.498759305211\n",
      "0.50682382134\n",
      "0.50682382134\n",
      "0.51364764268\n",
      "0.517990074442\n",
      "0.522952853598\n",
      "0.526054590571\n",
      "0.532878411911\n",
      "0.539081885856\n",
      "0.541563275434\n",
      "0.546526054591\n",
      "0.546526054591\n",
      "0.549627791563\n",
      "0.552729528536\n",
      "0.550868486352\n",
      "0.552109181141\n",
      "0.551488833747\n",
      "0.553349875931\n",
      "0.55459057072\n",
      "0.553970223325\n",
      "0.556451612903\n",
      "0.555831265509\n",
      "0.556451612903\n",
      "0.556451612903\n",
      "0.555831265509\n",
      "0.555831265509\n",
      "0.555210918114\n",
      "0.555210918114\n",
      "0.555210918114\n",
      "0.253722084367\n",
      "0.254962779156\n",
      "0.289081885856\n",
      "0.295905707196\n",
      "0.329404466501\n",
      "0.333746898263\n",
      "0.369727047146\n",
      "0.374689826303\n",
      "0.396401985112\n",
      "0.400124069479\n",
      "0.413771712159\n",
      "0.422456575682\n",
      "0.442307692308\n",
      "0.450992555831\n",
      "0.461538461538\n",
      "0.465260545906\n",
      "0.480769230769\n",
      "0.486972704715\n",
      "0.490074441687\n",
      "0.498138957816\n",
      "0.50682382134\n",
      "0.516749379653\n",
      "0.521091811414\n",
      "0.522952853598\n",
      "0.526054590571\n",
      "0.532878411911\n",
      "0.535359801489\n",
      "0.545905707196\n",
      "0.550248138958\n",
      "0.553970223325\n",
      "0.555831265509\n",
      "0.562034739454\n",
      "0.563895781638\n",
      "0.567617866005\n",
      "0.56699751861\n",
      "0.570719602978\n",
      "0.571960297767\n",
      "0.571960297767\n",
      "0.572580645161\n",
      "0.575062034739\n",
      "0.574441687345\n",
      "0.574441687345\n",
      "0.574441687345\n",
      "0.57382133995\n",
      "0.57382133995\n",
      "0.574441687345\n",
      "0.574441687345\n",
      "0.57382133995\n",
      "0.573200992556\n",
      "0.573200992556\n",
      "0.246898263027\n",
      "0.246898263027\n",
      "0.282878411911\n",
      "0.284739454094\n",
      "0.321960297767\n",
      "0.32382133995\n",
      "0.35794044665\n",
      "0.362903225806\n",
      "0.394540942928\n",
      "0.401364764268\n",
      "0.421215880893\n",
      "0.429900744417\n",
      "0.439205955335\n",
      "0.449751861042\n",
      "0.465260545906\n",
      "0.473945409429\n",
      "0.486972704715\n",
      "0.494416873449\n",
      "0.497518610422\n",
      "0.502481389578\n",
      "0.509305210918\n",
      "0.516749379653\n",
      "0.519851116625\n",
      "0.526054590571\n",
      "0.532878411911\n",
      "0.537220843672\n",
      "0.538461538462\n",
      "0.539702233251\n",
      "0.542183622829\n",
      "0.545285359801\n",
      "0.550868486352\n",
      "0.552109181141\n",
      "0.555831265509\n",
      "0.56141439206\n",
      "0.562655086849\n",
      "0.563895781638\n",
      "0.565756823821\n",
      "0.563275434243\n",
      "0.565136476427\n",
      "0.565756823821\n",
      "0.566377171216\n",
      "0.566377171216\n",
      "0.566377171216\n",
      "0.566377171216\n",
      "0.567617866005\n",
      "0.568858560794\n",
      "0.570099255583\n",
      "0.570099255583\n",
      "0.570099255583\n",
      "0.570099255583\n",
      "0.243796526055\n",
      "0.243796526055\n",
      "0.276674937965\n",
      "0.279156327543\n",
      "0.31699751861\n",
      "0.321339950372\n",
      "0.349875930521\n",
      "0.356079404467\n",
      "0.377791563275\n",
      "0.385856079404\n",
      "0.409429280397\n",
      "0.41935483871\n",
      "0.43300248139\n",
      "0.437344913151\n",
      "0.454094292804\n",
      "0.45905707196\n",
      "0.472084367246\n",
      "0.477667493797\n",
      "0.490694789082\n",
      "0.497518610422\n",
      "0.50682382134\n",
      "0.511786600496\n",
      "0.519851116625\n",
      "0.525434243176\n",
      "0.530397022333\n",
      "0.5341191067\n",
      "0.539081885856\n",
      "0.543424317618\n",
      "0.544665012407\n",
      "0.54776674938\n",
      "0.549627791563\n",
      "0.553970223325\n",
      "0.558933002481\n",
      "0.563275434243\n",
      "0.567617866005\n",
      "0.566377171216\n",
      "0.5682382134\n",
      "0.567617866005\n",
      "0.569478908189\n",
      "0.570719602978\n",
      "0.570099255583\n",
      "0.571339950372\n",
      "0.571960297767\n",
      "0.572580645161\n",
      "0.572580645161\n",
      "0.571960297767\n",
      "0.571339950372\n",
      "0.571339950372\n",
      "0.571339950372\n",
      "0.571339950372\n",
      "0.271091811414\n",
      "0.273573200993\n",
      "0.303349875931\n",
      "0.308312655087\n",
      "0.332506203474\n",
      "0.342431761787\n",
      "0.375930521092\n",
      "0.388957816377\n",
      "0.413771712159\n",
      "0.423697270471\n",
      "0.435483870968\n",
      "0.446029776675\n",
      "0.458436724566\n",
      "0.473325062035\n",
      "0.482009925558\n",
      "0.486972704715\n",
      "0.496898263027\n",
      "0.504962779156\n",
      "0.513027295285\n",
      "0.517369727047\n",
      "0.523573200993\n",
      "0.524813895782\n",
      "0.529156327543\n",
      "0.531637717122\n",
      "0.539081885856\n",
      "0.54094292804\n",
      "0.546526054591\n",
      "0.555210918114\n",
      "0.560794044665\n",
      "0.562034739454\n",
      "0.564516129032\n",
      "0.566377171216\n",
      "0.5682382134\n",
      "0.568858560794\n",
      "0.570719602978\n",
      "0.5682382134\n",
      "0.567617866005\n",
      "0.565136476427\n",
      "0.565136476427\n",
      "0.563895781638\n",
      "0.563895781638\n",
      "0.563275434243\n",
      "0.562655086849\n",
      "0.563275434243\n",
      "0.563895781638\n",
      "0.563895781638\n",
      "0.563895781638\n",
      "0.563895781638\n",
      "0.563275434243\n",
      "0.563275434243\n",
      "0.244416873449\n",
      "0.245657568238\n",
      "0.277915632754\n",
      "0.279776674938\n",
      "0.313275434243\n",
      "0.315136476427\n",
      "0.343672456576\n",
      "0.352977667494\n",
      "0.376550868486\n",
      "0.37841191067\n",
      "0.402605459057\n",
      "0.411290322581\n",
      "0.426799007444\n",
      "0.436104218362\n",
      "0.454094292804\n",
      "0.463399503722\n",
      "0.478287841191\n",
      "0.491935483871\n",
      "0.498138957816\n",
      "0.506203473945\n",
      "0.513027295285\n",
      "0.516129032258\n",
      "0.519851116625\n",
      "0.526674937965\n",
      "0.526054590571\n",
      "0.529776674938\n",
      "0.532878411911\n",
      "0.537220843672\n",
      "0.541563275434\n",
      "0.545285359801\n",
      "0.54776674938\n",
      "0.549627791563\n",
      "0.552109181141\n",
      "0.552109181141\n",
      "0.555831265509\n",
      "0.55459057072\n",
      "0.55459057072\n",
      "0.55459057072\n",
      "0.558312655087\n",
      "0.558312655087\n",
      "0.557692307692\n",
      "0.558312655087\n",
      "0.558312655087\n",
      "0.559553349876\n",
      "0.559553349876\n",
      "0.558933002481\n",
      "0.558312655087\n",
      "0.558312655087\n",
      "0.558312655087\n",
      "0.558312655087\n",
      "0.250620347395\n",
      "0.25\n",
      "0.276674937965\n",
      "0.279156327543\n",
      "0.316377171216\n",
      "0.317617866005\n",
      "0.347394540943\n",
      "0.352977667494\n",
      "0.373449131514\n",
      "0.377791563275\n",
      "0.400124069479\n",
      "0.41253101737\n",
      "0.429900744417\n",
      "0.437344913151\n",
      "0.447890818859\n",
      "0.455955334988\n",
      "0.465260545906\n",
      "0.468362282878\n",
      "0.481389578164\n",
      "0.491315136476\n",
      "0.497518610422\n",
      "0.508064516129\n",
      "0.512406947891\n",
      "0.522332506203\n",
      "0.533498759305\n",
      "0.537220843672\n",
      "0.540322580645\n",
      "0.542803970223\n",
      "0.544665012407\n",
      "0.54776674938\n",
      "0.548387096774\n",
      "0.549007444169\n",
      "0.547146401985\n",
      "0.549627791563\n",
      "0.552729528536\n",
      "0.552729528536\n",
      "0.55459057072\n",
      "0.553970223325\n",
      "0.555210918114\n",
      "0.556451612903\n",
      "0.557071960298\n",
      "0.557071960298\n",
      "0.557692307692\n",
      "0.558312655087\n",
      "0.558312655087\n",
      "0.558312655087\n",
      "0.558312655087\n",
      "0.558312655087\n",
      "0.558312655087\n",
      "0.558312655087\n",
      "0.256203473945\n",
      "0.256203473945\n",
      "0.289081885856\n",
      "0.292803970223\n",
      "0.32382133995\n",
      "0.327543424318\n",
      "0.354838709677\n",
      "0.359801488834\n",
      "0.382133995037\n",
      "0.396401985112\n",
      "0.408808933002\n",
      "0.415632754342\n",
      "0.434863523573\n",
      "0.442307692308\n",
      "0.455334987593\n",
      "0.465260545906\n",
      "0.475186104218\n",
      "0.486972704715\n",
      "0.49317617866\n",
      "0.498759305211\n",
      "0.502481389578\n",
      "0.508684863524\n",
      "0.515508684864\n",
      "0.519230769231\n",
      "0.521712158809\n",
      "0.525434243176\n",
      "0.529776674938\n",
      "0.531017369727\n",
      "0.537220843672\n",
      "0.539081885856\n",
      "0.538461538462\n",
      "0.539081885856\n",
      "0.544665012407\n",
      "0.544044665012\n",
      "0.549627791563\n",
      "0.552109181141\n",
      "0.552109181141\n",
      "0.551488833747\n",
      "0.552729528536\n",
      "0.550868486352\n",
      "0.552109181141\n",
      "0.550868486352\n",
      "0.552109181141\n",
      "0.552729528536\n",
      "0.552729528536\n",
      "0.553349875931\n",
      "0.553349875931\n",
      "0.553349875931\n",
      "0.553349875931\n",
      "0.55459057072\n",
      "0.247518610422\n",
      "0.247518610422\n",
      "0.273573200993\n",
      "0.276674937965\n",
      "0.309553349876\n",
      "0.313275434243\n",
      "0.341811414392\n",
      "0.344913151365\n",
      "0.373449131514\n",
      "0.384615384615\n",
      "0.403846153846\n",
      "0.415012406948\n",
      "0.434243176179\n",
      "0.43982630273\n",
      "0.453473945409\n",
      "0.45905707196\n",
      "0.472084367246\n",
      "0.480148883375\n",
      "0.482009925558\n",
      "0.490074441687\n",
      "0.497518610422\n",
      "0.501240694789\n",
      "0.505583126551\n",
      "0.508684863524\n",
      "0.51364764268\n",
      "0.514267990074\n",
      "0.517369727047\n",
      "0.522332506203\n",
      "0.523573200993\n",
      "0.526054590571\n",
      "0.529156327543\n",
      "0.532258064516\n",
      "0.531637717122\n",
      "0.5341191067\n",
      "0.538461538462\n",
      "0.539081885856\n",
      "0.542183622829\n",
      "0.542803970223\n",
      "0.544044665012\n",
      "0.545285359801\n",
      "0.545905707196\n",
      "0.545285359801\n",
      "0.544044665012\n",
      "0.544665012407\n",
      "0.544044665012\n",
      "0.544044665012\n",
      "0.544665012407\n",
      "0.544665012407\n",
      "0.544665012407\n",
      "0.544665012407\n",
      "0.251861042184\n",
      "0.251861042184\n",
      "0.277915632754\n",
      "0.281637717122\n",
      "0.315756823821\n",
      "0.320099255583\n",
      "0.354218362283\n",
      "0.35794044665\n",
      "0.385856079404\n",
      "0.390818858561\n",
      "0.413771712159\n",
      "0.418114143921\n",
      "0.429280397022\n",
      "0.43858560794\n",
      "0.450372208437\n",
      "0.454094292804\n",
      "0.464640198511\n",
      "0.473325062035\n",
      "0.480148883375\n",
      "0.48635235732\n",
      "0.496277915633\n",
      "0.503101736973\n",
      "0.508064516129\n",
      "0.513027295285\n",
      "0.522952853598\n",
      "0.525434243176\n",
      "0.533498759305\n",
      "0.536600496278\n",
      "0.539081885856\n",
      "0.539702233251\n",
      "0.542183622829\n",
      "0.543424317618\n",
      "0.547146401985\n",
      "0.552729528536\n",
      "0.555210918114\n",
      "0.558312655087\n",
      "0.56017369727\n",
      "0.558312655087\n",
      "0.560794044665\n",
      "0.56017369727\n",
      "0.56017369727\n",
      "0.56017369727\n",
      "0.558933002481\n",
      "0.558312655087\n",
      "0.559553349876\n",
      "0.559553349876\n",
      "0.559553349876\n",
      "0.558933002481\n",
      "0.558933002481\n",
      "0.558933002481\n",
      "0.26364764268\n",
      "0.26364764268\n",
      "0.296526054591\n",
      "0.299627791563\n",
      "0.332506203474\n",
      "0.33746898263\n",
      "0.373449131514\n",
      "0.377171215881\n",
      "0.404466501241\n",
      "0.413771712159\n",
      "0.431141439206\n",
      "0.442307692308\n",
      "0.457816377171\n",
      "0.462779156328\n",
      "0.477047146402\n",
      "0.483250620347\n",
      "0.491315136476\n",
      "0.499379652605\n",
      "0.50682382134\n",
      "0.512406947891\n",
      "0.52047146402\n",
      "0.524193548387\n",
      "0.531637717122\n",
      "0.538461538462\n",
      "0.54094292804\n",
      "0.54776674938\n",
      "0.545905707196\n",
      "0.550868486352\n",
      "0.553970223325\n",
      "0.557692307692\n",
      "0.559553349876\n",
      "0.558933002481\n",
      "0.559553349876\n",
      "0.559553349876\n",
      "0.562034739454\n",
      "0.565136476427\n",
      "0.570099255583\n",
      "0.56699751861\n",
      "0.568858560794\n",
      "0.569478908189\n",
      "0.5682382134\n",
      "0.568858560794\n",
      "0.569478908189\n",
      "0.570099255583\n",
      "0.570719602978\n",
      "0.570719602978\n",
      "0.570719602978\n",
      "0.570719602978\n",
      "0.570719602978\n",
      "0.570719602978\n",
      "0.263027295285\n",
      "0.263027295285\n",
      "0.289081885856\n",
      "0.291563275434\n",
      "0.326923076923\n",
      "0.328784119107\n",
      "0.362903225806\n",
      "0.369727047146\n",
      "0.392679900744\n",
      "0.399503722084\n",
      "0.424317617866\n",
      "0.4317617866\n",
      "0.447890818859\n",
      "0.452853598015\n",
      "0.469602977667\n",
      "0.474565756824\n",
      "0.482009925558\n",
      "0.48635235732\n",
      "0.496898263027\n",
      "0.499379652605\n",
      "0.505583126551\n",
      "0.508684863524\n",
      "0.516749379653\n",
      "0.517990074442\n",
      "0.524813895782\n",
      "0.526674937965\n",
      "0.535980148883\n",
      "0.537841191067\n",
      "0.536600496278\n",
      "0.538461538462\n",
      "0.539081885856\n",
      "0.541563275434\n",
      "0.546526054591\n",
      "0.547146401985\n",
      "0.548387096774\n",
      "0.549007444169\n",
      "0.549627791563\n",
      "0.552109181141\n",
      "0.553970223325\n",
      "0.556451612903\n",
      "0.555831265509\n",
      "0.555210918114\n",
      "0.555210918114\n",
      "0.55459057072\n",
      "0.55459057072\n",
      "0.55459057072\n",
      "0.553970223325\n",
      "0.553970223325\n",
      "0.553349875931\n",
      "0.553349875931\n",
      "[0.25502481389578163, 0.25570719602977665, 0.28718982630272955, 0.29010545905707191, 0.32285980148883375, 0.32686104218362277, 0.35694789081885864, 0.36395781637717117, 0.38833746898263033, 0.39633995037220848, 0.41628411910669982, 0.42555831265508681, 0.44063275434243171, 0.44922456575682385, 0.46181761786600484, 0.47019230769230769, 0.481048387096774, 0.48858560794044664, 0.49612282878411912, 0.50279156327543417, 0.51088709677419364, 0.51653225806451597, 0.52223945409429284, 0.52710918114143923, 0.53185483870967754, 0.53554590570719607, 0.53920595533498761, 0.54283498759305204, 0.54540942928039704, 0.5486042183622829, 0.55074441687344911, 0.55282258064516121, 0.55452853598014884, 0.55676178660049636, 0.55911910669975196, 0.55980148883374681, 0.56110421836228286, 0.56110421836228286, 0.56218982630272962, 0.56274813895781639, 0.56284119106699748, 0.56299627791563267, 0.56290322580645169, 0.56293424317617879, 0.56305831265508677, 0.56305831265508699, 0.56305831265508688, 0.56299627791563278, 0.56287220843672459, 0.56293424317617868]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = [0]*50\n",
    "for j in range(20):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dat['words_clean'], dat['label'], test_size=0.3, random_state=j)\n",
    "    rf_model.fit(X_train, y_train) \n",
    "    threshes = np.arange(0.0, 0.5, 0.01)\n",
    "    for i, thresh in enumerate(threshes):\n",
    "        probs = rf_model.predict_proba(X_test)\n",
    "        acc = return_acc(probs, y_test, thresh)[1]\n",
    "        print(acc)\n",
    "        accuracy[i] += acc\n",
    "\n",
    "new_acc = [acc/20 for acc in accuracy]\n",
    "print(new_acc)\n",
    "np.argmax(new_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56299627791563267"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_acc[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_feats = [10,30,50,100,200,300,500,800,1000,1500,2000,2500,3000,3500,4000]\n",
    "avg_acc = [None]*len(num_feats)\n",
    "for i, feats in enumerate(num_feats):\n",
    "    rf_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,1))), \n",
    "        ('tfidf', TfidfTransformer()), ('clf', ensemble.RandomForestClassifier(max_depth=500, max_features=feats, n_estimators=10)),])\n",
    "    #print(nb_model_2)\n",
    "    accuracy = [0]*20\n",
    "    for j in range(len(accuracy)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dat['words_clean'], dat['label'], test_size=0.3, random_state=j)\n",
    "        rf_model.fit(X_train, y_train) \n",
    "        probs = rf_model.predict_proba(X_test)\n",
    "        accuracy[j] += return_acc(probs, y_test, 0.28)[1]\n",
    "    avg_acc[i] = sum(accuracy)/len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_feats[np.argmax(avg_acc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "depths = [1,2,3,5,10,30,50,70,100,200,300,500,800,1000,1500]\n",
    "avg_acc = [None]*len(depths)\n",
    "for i, depth in enumerate(depths):\n",
    "    rf_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,1))), \n",
    "        ('tfidf', TfidfTransformer()), ('clf', ensemble.RandomForestClassifier(max_depth=depth, max_features=1500, n_estimators=10)),])\n",
    "    #print(nb_model_2)\n",
    "    accuracy = [0]*20\n",
    "    for j in range(len(accuracy)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dat['words_clean'], dat['label'], test_size=0.3, random_state=j)\n",
    "        rf_model.fit(X_train, y_train) \n",
    "        probs = rf_model.predict_proba(X_test)\n",
    "        accuracy[j] += return_acc(probs, y_test, 0.28)[1]\n",
    "    avg_acc[i] = sum(accuracy)/len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depths[np.argmax(avg_acc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_estimators = [1,2,3,5,10,20,30]\n",
    "avg_acc = [None]*len(num_estimators)\n",
    "for i, n in enumerate(num_estimators):\n",
    "    rf_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,1))), \n",
    "        ('tfidf', TfidfTransformer()), ('clf', ensemble.RandomForestClassifier(max_depth=200, max_features=1500, n_estimators=n)),])\n",
    "    #print(nb_model_2)\n",
    "    accuracy = [0]*20\n",
    "    for j in range(len(accuracy)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dat['words_clean'], dat['label'], test_size=0.3, random_state=j)\n",
    "        rf_model.fit(X_train, y_train) \n",
    "        probs = rf_model.predict_proba(X_test)\n",
    "        accuracy[j] += return_acc(probs, y_test, 0.28)[1]\n",
    "    avg_acc[i] = sum(accuracy)/len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.48979528535980144,\n",
       " 0.48874069478908178,\n",
       " 0.47732630272952853,\n",
       " 0.51913771712158796,\n",
       " 0.52679900744416874,\n",
       " 0.5367866004962778,\n",
       " 0.5436104218362281]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_estimators[np.argmax(avg_acc)]\n",
    "avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,1))), \n",
    "        ('tfidf', TfidfTransformer()), ('clf', ensemble.RandomForestClassifier(max_depth=200, max_features=1500, n_estimators=50)),])\n",
    "accuracy = [0]*20\n",
    "for j in range(len(accuracy)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dat['words_clean'], dat['label'], test_size=0.3, random_state=j)\n",
    "    rf_model.fit(X_train, y_train) \n",
    "    probs = rf_model.predict_proba(X_test)\n",
    "    accuracy[j] += return_acc(probs, y_test, 0.28)[1]\n",
    "avg_acc = sum(accuracy)/len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54655707196029779"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,1))), \n",
    "        ('tfidf', TfidfTransformer()), ('clf', ensemble.RandomForestClassifier(max_depth=200, max_features=1500, n_estimators=50)),])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
